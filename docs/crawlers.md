# ğŸ•·ï¸ Crawlers Subsystem

The crawlers subsystem (`src/crawlers/`) is the core web crawling engine that discovers and extracts product data from major retail websites. It provides a unified interface for crawling Amazon, Target, and Walmart with advanced features like UPC lookup, category hierarchies, and multiple output backends.

## ğŸ—ï¸ Architecture Overview

```
src/crawlers/
â”œâ”€â”€ base_crawler.py          # ğŸ§± Base crawler classes and interfaces
â”œâ”€â”€ supabase_backend.py      # ğŸ—„ï¸ Supabase database integration
â”œâ”€â”€ upc_lookup/              # ğŸ” UPC/barcode lookup services
â”‚   â”œâ”€â”€ base_upc_lookup.py   # ğŸ“‹ Abstract UPC lookup interface
â”‚   â”œâ”€â”€ barcode_lookup.py    # ğŸ·ï¸ BarcodeLookup.com integration
â”‚   â””â”€â”€ upc_manager.py       # ğŸ¯ Multi-service UPC management
â”œâ”€â”€ normalizers/             # ğŸ”§ Category and data normalization
â”œâ”€â”€ amazon/                  # ğŸ“¦ Amazon-specific crawler
â”œâ”€â”€ target/                  # ğŸ¯ Target-specific crawler
â””â”€â”€ walmart/                 # ğŸª Walmart-specific crawler
```

## ğŸ”§ Core Components

### ğŸ§± Base Crawler (`base_crawler.py`)

The foundation for all retailer-specific crawlers, providing:
- âœ¨ Abstract base classes defining crawler interfaces
- ğŸ› ï¸ Common crawling patterns and utilities
- ğŸ›¡ï¸ Error handling and retry mechanisms
- ğŸ”— Session management for web scraping
- ğŸ“Š Category hierarchy processing

**ğŸ”‘ Key Classes:**
- `BaseCrawler`: Abstract base for all crawlers
- `CategoryCrawler`: Specialized for category-based crawling
- `ProductCrawler`: Specialized for product data extraction

### ğŸ” UPC Lookup System (`upc_lookup/`)

A sophisticated system for enriching product data with UPC/barcode information:

#### âœ¨ Features:
- ğŸ”„ **Multi-service fallback**: Automatically tries multiple UPC lookup services
- ğŸ§  **Intelligent caching**: Reduces redundant API calls with smart caching
- ğŸ“Š **Confidence scoring**: Evaluates reliability of UPC matches
- ğŸ¯ **Fuzzy matching**: Uses product name similarity for UPC discovery
- â±ï¸ **Rate limiting**: Respects API limits and handles throttling

#### ğŸ—ï¸ Components:
- **`base_upc_lookup.py`**: Abstract interface for UPC lookup services
- **`barcode_lookup.py`**: BarcodeLookup.com API integration with fuzzy matching
- **`upc_manager.py`**: Orchestrates multiple UPC services with failover

#### ğŸ’¡ Usage Example:
```python
from src.crawlers.upc_lookup.upc_manager import UPCManager

upc_manager = UPCManager()
upc_data = await upc_manager.lookup_upc_by_name("Coca Cola 12oz")
if upc_data:
    print(f"UPC: {upc_data['upc']}, Confidence: {upc_data['confidence']}")
```

### ğŸ—„ï¸ Supabase Backend (`supabase_backend.py`)

Provides direct database integration for storing crawled data:

#### âœ¨ Features:
- ğŸ“Š **Structured data storage**: Organized tables for products, categories, brands, etc.
- ğŸš« **Automatic deduplication**: Prevents duplicate entries across crawls
- ğŸ“ˆ **Price tracking**: Historical price data for trend analysis
- ğŸ·ï¸ **Category normalization**: Standardizes category names across retailers
- ğŸ” **UPC enrichment**: Automatically enhances products with UPC data

#### ğŸ—ƒï¸ Database Schema:
- `products`: Core product information
- `listings`: Retailer-specific product listings
- `categories`: Normalized category hierarchy
- `brands`: Brand information and normalization
- `price_histories`: Historical pricing data
- `upcs`: UPC/barcode mappings

## ğŸ›ï¸ Retailer-Specific Crawlers

### ğŸ“¦ Amazon Crawler (`amazon/`)
- ğŸŒ³ Category hierarchy discovery
- ğŸ“Š Product grid crawling
- ğŸ” Search result processing
- ğŸ¯ Amazon-specific data extraction patterns
- ğŸ¢ Department-based organization

### ğŸ¯ Target Crawler (`target/`)
- ğŸŒ Target.com category navigation
- ğŸ“„ Product page scraping
- ğŸ’° Price and availability tracking
- ğŸ†” Target-specific identifiers

### ğŸª Walmart Crawler (`walmart/`)
- ğŸŒ Walmart.com product discovery
- ğŸ“Š Category-based crawling
- ğŸ¯ Walmart-specific data patterns
- ğŸŒ Regional availability tracking

## ğŸ“‹ Crawling Modes

### ğŸ” Full Mode
Extracts complete product information:
- ğŸ“ Product title and description
- ğŸ’° Price and availability
- ğŸ”— Product URLs and images
- ğŸ†” Retailer-specific IDs
- ğŸ·ï¸ UPC codes (when available)
- ğŸ“‚ Category classifications

### ğŸ”— URLs-Only Mode
Memory-efficient URL collection:
- ğŸ¯ Extracts only product URLs
- âš¡ Optimized for batch processing
- ğŸ’¾ Lower memory footprint
- ğŸš€ Faster category traversal

### ğŸŒ³ Hierarchical Mode
Builds complete category structures:
- ğŸ“Š Nested category hierarchies
- ğŸ“ Products organized by category
- ğŸ“„ JSON output with tree structure
- ğŸ¯ Comprehensive category coverage

## âš™ï¸ Configuration and Customization

### ğŸŒ Environment Variables
```bash
CRAWLER_MAX_DEPTH=10          # ğŸ“ Maximum crawling depth
CRAWLER_CONCURRENCY=5         # âš¡ Concurrent crawlers
UPC_LOOKUP_ENABLED=true       # ğŸ” Enable UPC enrichment
UPC_CACHE_SIZE=10000         # ğŸ’¾ UPC cache size
```

### ğŸ“‚ Category Configuration
Each retailer has customizable category configurations:
- ğŸ—ºï¸ Department mappings
- ğŸ”— Category URL patterns
- â­ Crawling priorities
- ğŸš« Exclusion rules

### ğŸš€ Performance Tuning
- âš¡ **Concurrency levels**: Adjust based on system resources
- â±ï¸ **Rate limiting**: Respect retailer robots.txt and ToS
- ğŸ§  **Caching strategies**: Optimize for repeated crawls
- ğŸ’¾ **Memory management**: Handle large datasets efficiently

## ğŸ›¡ï¸ Error Handling and Resilience

### ğŸ”„ Retry Mechanisms
- ğŸ” Automatic retry for transient failures
- ğŸ“ˆ Exponential backoff for rate limiting
- âš¡ Circuit breaker patterns for service failures
- ğŸŒŸ Graceful degradation when services unavailable

### ğŸ“Š Monitoring and Logging
- ğŸ“ Comprehensive logging at multiple levels
- â±ï¸ Performance metrics and timing
- ğŸš¨ Error tracking and categorization
- ğŸ“ˆ Progress reporting for long-running crawls

### âœ… Data Quality Assurance
- âœ”ï¸ Product data validation
- ğŸš« Duplicate detection and handling
- ğŸ” Consistency checks across retailers
- ğŸ·ï¸ UPC validation and verification

## ğŸ”Œ Integration Points

### ğŸ“® Redis Integration
- ğŸ“‹ URL queue management
- ğŸŒ Distributed crawling coordination
- ğŸ’¾ Result caching and storage
- ğŸ“Š Progress tracking across workers

### ğŸ“„ JSON File Output
- ğŸ“Š Structured data export
- ğŸŒ³ Hierarchical organization
- ğŸ”„ Batch processing support
- ğŸ‘ï¸ Human-readable formats

### ğŸ”Œ API Integrations
- ğŸ” UPC lookup services
- ğŸ’° Price tracking APIs
- ğŸ“‚ Category mapping services
- âœ¨ Product enrichment APIs

## ğŸ“– Best Practices

### ğŸ‘¨â€ğŸ’» Development Guidelines
1. **ğŸ§± Extend base classes**: Use `BaseCrawler` for new retailers
2. **ğŸ”Œ Implement interfaces**: Follow UPC lookup patterns for new services
3. **â±ï¸ Handle rate limits**: Respect retailer terms of service
4. **ğŸ§ª Test thoroughly**: Validate against real retailer sites
5. **ğŸ“š Document patterns**: Maintain clear crawler documentation

### ğŸ—ï¸ Operational Guidelines
1. **ğŸ“Š Monitor performance**: Track crawling speed and success rates
2. **âš–ï¸ Manage resources**: Balance concurrency with system capacity
3. **âœ… Validate data**: Regularly check data quality and completeness
4. **ğŸ”„ Update patterns**: Keep crawler patterns current with site changes
5. **ğŸ’¾ Backup configurations**: Maintain version control for configurations

## ğŸ”§ Troubleshooting

### âš ï¸ Common Issues
- **â±ï¸ Rate limiting**: Reduce concurrency or add delays
- **ğŸ”„ Site changes**: Update crawler patterns for layout changes
- **ğŸ” UPC lookup failures**: Check API keys and service status
- **ğŸ’¾ Memory issues**: Use URLs-only mode for large crawls
- **ğŸ—„ï¸ Database errors**: Verify Supabase connection and schema

### ğŸ› ï¸ Debug Tools
```bash
# ğŸ” Enable debug logging
python scripts/crawl.py --retailer amazon --log-level DEBUG

# ğŸ§ª Test specific components
python scripts/crawl.py --test-redis
python scripts/crawl.py --test-upc-lookup

# âœ… Validate configurations
python scripts/crawl.py --validate-config
``` 