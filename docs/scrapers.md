# ğŸ” Scrapers Subsystem

The scrapers subsystem (`src/scrapers/`) provides direct product page scraping capabilities as an alternative to the crawler-based approach. While crawlers discover products through category navigation, scrapers work with specific product URLs to extract detailed product information directly from individual product pages.

## ğŸ—ï¸ Architecture Overview

```
src/scrapers/
â”œâ”€â”€ base_scraper.py     # ğŸ§± Base scraper class and interfaces
â”œâ”€â”€ __init__.py         # ğŸ”§ Scraper module initialization
â”œâ”€â”€ amazon/             # ğŸ“¦ Amazon-specific product scraper
â”œâ”€â”€ target/             # ğŸ¯ Target-specific product scraper
â””â”€â”€ walmart/            # ğŸª Walmart-specific product scraper
```

## ğŸ’¡ Core Concepts

### ğŸ” Scrapers vs ğŸ•·ï¸ Crawlers

| Feature | ğŸ” Scrapers | ğŸ•·ï¸ Crawlers |
|---------|-------------|-------------|
| **ğŸ¯ Purpose** | Extract data from known product URLs | Discover and extract from categories |
| **ğŸ“¥ Input** | Specific product URLs | Category URLs or hierarchy |
| **âš¡ Speed** | Fast for individual products | Optimized for bulk discovery |
| **ğŸ¯ Use Case** | Targeted product updates | Comprehensive product discovery |
| **ğŸ’¾ Memory Usage** | Low per product | Higher for bulk operations |

### ğŸ¯ When to Use Scrapers

1. **ğŸ”„ Product Updates**: Refreshing data for known products
2. **ğŸ’° Price Monitoring**: Regular price checks for specific items
3. **ğŸ“¦ Inventory Tracking**: Checking availability for targeted products
4. **âœ… Data Validation**: Verifying crawler results for specific products
5. **âš¡ Real-time Data**: Getting current information for immediate use

## ğŸ§± Base Scraper (`base_scraper.py`)

The foundation for all retailer-specific scrapers, providing:

### âœ¨ Core Features
- ğŸ—ï¸ **Abstract base class**: Standardized interface for all scrapers
- ğŸ›¡ï¸ **Error handling**: Robust error handling and retry mechanisms
- ğŸ”— **Session management**: Efficient HTTP session handling
- ğŸ” **Response parsing**: Common parsing utilities and patterns
- â±ï¸ **Rate limiting**: Built-in rate limiting and respectful scraping

### ğŸ”‘ Key Methods
```python
class BaseScraper:
    def scrape_product(self, url: str) -> dict:
        """ğŸ” Scrape a single product from its URL"""
        pass
    
    def scrape_multiple(self, urls: list) -> list:
        """ğŸ“¦ Scrape multiple products with optimized batching"""
        pass
    
    def validate_url(self, url: str) -> bool:
        """âœ… Validate if URL belongs to supported retailer"""
        pass
```

### ğŸ› ï¸ Common Patterns
- ğŸ”„ **User-agent rotation**: Prevents blocking through varied headers
- ğŸ’¾ **Response caching**: Reduces redundant requests for debugging
- ğŸ“ **HTML parsing**: BeautifulSoup and lxml integration
- ğŸ“Š **JSON-LD extraction**: Structured data extraction from product pages
- ğŸ–¼ï¸ **Image processing**: Product image URL extraction and validation

## ğŸ›ï¸ Retailer-Specific Scrapers

### ğŸ“¦ Amazon Scraper (`amazon/`)

Specialized for Amazon product pages with:

#### âœ¨ Key Features
- ğŸ†” **ASIN extraction**: Amazon Standard Identification Numbers
- ğŸ’° **Price parsing**: Regular and sale price detection
- â­ **Review extraction**: Rating scores and review counts
- ğŸ¨ **Variant handling**: Size, color, and other product variants
- ğŸ“¦ **Availability detection**: In-stock status and shipping info

#### ğŸ“Š Data Points Extracted
```python
{
    "title": "ğŸ“ Product name and description",
    "price": "ğŸ’° Current price with currency",
    "original_price": "ğŸ’¸ List price before discounts",
    "asin": "ğŸ†” Amazon Standard Identification Number",
    "rating": "â­ Average customer rating",
    "review_count": "ğŸ’¬ Number of customer reviews",
    "availability": "ğŸ“¦ Stock status and shipping",
    "images": ["ğŸ–¼ï¸ List of product image URLs"],
    "brand": "ğŸ¢ Product brand name",
    "categories": ["ğŸ“‚ Product category hierarchy"],
    "features": ["âœ¨ Key product features and bullets"],
    "description": "ğŸ“‹ Detailed product description"
}
```

#### ğŸ”’ Special Handling
- ğŸ¤– **Anti-bot detection**: Strategies for avoiding Amazon's bot detection
- ğŸŒ **Regional variations**: Handling different Amazon domains
- ğŸ‘‘ **Prime pricing**: Special handling for Prime member pricing
- ğŸ“… **Subscribe & Save**: Subscription pricing detection

### ğŸ¯ Target Scraper (`target/`)

Optimized for Target.com product pages:

#### âœ¨ Key Features
- ğŸ†” **TCIN extraction**: Target.com Item Numbers
- ğŸª **Store inventory**: Local store availability checking
- ğŸ¯ **Circle pricing**: Target Circle member pricing
- ğŸš— **Pickup options**: Store pickup and drive-up availability
- ğŸ“‹ **Product specifications**: Detailed spec table extraction

#### ğŸ“Š Data Points Extracted
```python
{
    "title": "ğŸ“ Product name",
    "price": "ğŸ’° Current price",
    "tcin": "ğŸ†” Target.com Item Number",
    "dpci": "ğŸ”¢ Department-Class-Item number",
    "upc": "ğŸ·ï¸ Universal Product Code",
    "brand": "ğŸ¢ Product brand",
    "rating": "â­ Guest rating average",
    "review_count": "ğŸ’¬ Number of reviews",
    "availability": "ğŸ“¦ Online and store availability",
    "store_pickup": "ğŸª Pickup availability by store",
    "specifications": {"ğŸ” key": "value pairs of product specs"},
    "images": ["ğŸ–¼ï¸ Product image URLs"],
    "categories": ["ğŸ“‚ Category breadcrumbs"]
}
```

#### ğŸ¯ Special Features
- ğŸ“ **Store locator integration**: Finding nearby stores with inventory
- ğŸ‘¥ **Guest reviews**: Target's guest review system parsing
- ğŸ·ï¸ **Promotion detection**: Sale badges and promotional pricing
- ğŸ’’ **Registry compatibility**: Wedding and baby registry features

### ğŸª Walmart Scraper (`walmart/`)

Designed for Walmart.com product extraction:

#### âœ¨ Key Features
- ğŸ†” **Item ID extraction**: Walmart product identifiers
- ğŸ”„ **Rollback pricing**: Walmart's rollback price detection
- ğŸš— **Pickup/delivery**: Store pickup and delivery options
- ğŸª **Third-party sellers**: Marketplace seller information
- ğŸ¨ **Product variants**: Size, color, and quantity variations

#### ğŸ“Š Data Points Extracted
```python
{
    "title": "ğŸ“ Product name",
    "price": "ğŸ’° Current price",
    "item_id": "ğŸ†” Walmart item identifier",
    "upc": "ğŸ·ï¸ Universal Product Code",
    "brand": "ğŸ¢ Product brand",
    "rating": "â­ Customer rating",
    "review_count": "ğŸ’¬ Number of reviews",
    "availability": "ğŸ“¦ Stock status",
    "seller": "ğŸª Walmart or marketplace seller",
    "shipping": "ğŸšš Shipping options and costs",
    "pickup_delivery": "ğŸª Store services availability",
    "specifications": {"ğŸ“‹ Product specifications"},
    "images": ["ğŸ–¼ï¸ Product images"],
    "categories": ["ğŸ“‚ Category hierarchy"]
}
```

## ğŸ’» Usage Patterns

### ğŸ” Single Product Scraping
```python
from src.scrapers.amazon.amazon_scraper import AmazonScraper

scraper = AmazonScraper()
product_data = scraper.scrape_product("https://amazon.com/dp/B08N5WRWNW")
print(f"ğŸ“ Product: {product_data['title']}")
print(f"ğŸ’° Price: {product_data['price']}")
```

### ğŸ“¦ Batch Scraping
```python
from src.scrapers.target.target_scraper import TargetScraper

urls = [
    "https://target.com/p/item1",
    "https://target.com/p/item2",
    "https://target.com/p/item3"
]

scraper = TargetScraper()
products = scraper.scrape_multiple(urls, batch_size=5)
for product in products:
    print(f"ğŸ“ {product['title']}: ğŸ’° {product['price']}")
```

### ğŸ›¡ï¸ Error Handling
```python
from src.scrapers.walmart.walmart_scraper import WalmartScraper

scraper = WalmartScraper()
try:
    product = scraper.scrape_product(url)
    if product:
        print("âœ… Successfully scraped product")
    else:
        print("âŒ Product not found or unavailable")
except Exception as e:
    print(f"âš ï¸ Scraping error: {e}")
```

## âš™ï¸ Configuration and Customization

### ğŸŒ Environment Variables
```bash
SCRAPER_USER_AGENT="ğŸ¤– Custom user agent string"
SCRAPER_TIMEOUT=30               # â±ï¸ Request timeout in seconds
SCRAPER_RETRY_COUNT=3           # ğŸ”„ Number of retry attempts
SCRAPER_RATE_LIMIT=1.0          # â±ï¸ Delay between requests
SCRAPER_CACHE_ENABLED=true      # ğŸ’¾ Enable response caching
```

### ğŸ”§ Custom Headers
```python
custom_headers = {
    'User-Agent': 'ğŸ¤– Mozilla/5.0 (compatible; ProductScraper/1.0)',
    'Accept': 'ğŸ“„ text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ğŸŒ en-US,en;q=0.5',
    'Accept-Encoding': 'ğŸ“¦ gzip, deflate',
    'Connection': 'ğŸ”— keep-alive'
}

scraper = AmazonScraper(headers=custom_headers)
```

### ğŸŒ Proxy Support
```python
proxy_config = {
    'http': 'ğŸŒ http://proxy-server:port',
    'https': 'ğŸ”’ https://proxy-server:port'
}

scraper = TargetScraper(proxies=proxy_config)
```

## ğŸš€ Performance Optimization

### âš¡ Concurrent Scraping
```python
import asyncio
from src.scrapers.base_scraper import AsyncBaseScraper

async def scrape_products_async(urls):
    scraper = AsyncBaseScraper()
    tasks = [scraper.scrape_product(url) for url in urls]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results
```

### ğŸ’¾ Response Caching
- ğŸ§  **Memory caching**: Fast access for repeated URLs
- ğŸ’¾ **File-based caching**: Persistent cache across sessions
- ğŸ—‘ï¸ **Cache invalidation**: Time-based and manual cache clearing
- ğŸ¯ **Selective caching**: Cache only successful responses

### ğŸ”§ Request Optimization
- ğŸ”— **Connection pooling**: Reuse HTTP connections
- ğŸ“¦ **Compression**: GZIP response handling
- ğŸ’š **Keep-alive**: Persistent connections for batch operations
- ğŸŒ **DNS caching**: Reduce DNS lookup overhead

## âœ… Data Quality and Validation

### ğŸ” Product Data Validation
```python
def validate_product_data(product):
    required_fields = ['title', 'price', 'url']
    for field in required_fields:
        if not product.get(field):
            raise ValueError(f"âŒ Missing required field: {field}")
    
    if not isinstance(product['price'], (int, float, str)):
        raise ValueError("âŒ Invalid price format")
    
    return True
```

### ğŸ’° Price Parsing
- ğŸ’± **Currency detection**: Automatic currency symbol handling
- ğŸ”¢ **Decimal formatting**: Proper decimal place handling
- ğŸ·ï¸ **Sale price extraction**: Regular vs. discounted pricing
- ğŸ“Š **Range pricing**: Handling price ranges for variants

### ğŸ–¼ï¸ Image URL Validation
- ğŸ”— **URL format checking**: Valid HTTP/HTTPS URLs
- âœ… **Image accessibility**: Verify images are accessible
- ğŸ“ **Size detection**: Extract image dimensions when available
- ğŸ”„ **Fallback images**: Handle missing or broken images

## ğŸ›¡ï¸ Error Handling and Resilience

### âš ï¸ Common Error Types
1. **ğŸŒ Network errors**: Connection timeouts and failures
2. **â±ï¸ Rate limiting**: HTTP 429 responses and blocking
3. **ğŸ”„ Page structure changes**: HTML layout modifications
4. **âŒ Missing products**: Discontinued or unavailable items
5. **ğŸŒ Geographic restrictions**: Region-locked content

### ğŸ”„ Retry Strategies
```python
import time
import random

def retry_with_backoff(func, max_retries=3):
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            delay = (2 ** attempt) + random.uniform(0, 1)
            time.sleep(delay)
```

### ğŸŒŸ Graceful Degradation
- ğŸ“‹ **Partial data extraction**: Return available data when some fields fail
- ğŸ”„ **Fallback parsing**: Alternative extraction methods
- ğŸ“ **Error reporting**: Detailed error logging for debugging
- ğŸ”§ **Recovery mechanisms**: Automatic retry and recovery

## ğŸ”Œ Integration with Other Subsystems

### ğŸ•·ï¸ Crawler Integration
- ğŸ”— **URL feeding**: Scrapers process URLs discovered by crawlers
- âœ… **Data validation**: Cross-verify crawler results with scraper data
- ğŸ”„ **Update cycles**: Regular updates for products found by crawlers

### ğŸ—„ï¸ Database Integration
- ğŸ’¾ **Direct storage**: Store scraped data directly to database
- ğŸ“¦ **Batch operations**: Efficient bulk data insertion
- âš–ï¸ **Conflict resolution**: Handle duplicate products and updates
- ğŸ“ˆ **History tracking**: Maintain product change history

### ğŸ·ï¸ UPC Enrichment
- ğŸ” **UPC extraction**: Extract UPC codes from product pages
- âœ… **Validation**: Verify UPC format and checksums
- ğŸ”Œ **Lookup integration**: Enhance with external UPC services
- ğŸ“Š **Confidence scoring**: Rate UPC extraction reliability

## ğŸ“– Best Practices

### ğŸ‘¨â€ğŸ’» Development Guidelines
1. **ğŸ“„ Respect robots.txt**: Check and follow site scraping policies
2. **â±ï¸ Rate limiting**: Implement appropriate delays between requests
3. **ğŸ¤– User-agent identification**: Use descriptive and honest user agents
4. **ğŸ›¡ï¸ Error handling**: Robust error handling for all scenarios
5. **âœ… Data validation**: Validate all extracted data before use

### ğŸ—ï¸ Operational Guidelines
1. **ğŸ“Š Monitor success rates**: Track scraping success and failure rates
2. **ğŸ”„ Update selectors**: Keep CSS selectors current with site changes
3. **ğŸ§ª Test regularly**: Validate scrapers against live sites
4. **ğŸ“ Log comprehensively**: Detailed logging for debugging and monitoring
5. **ğŸ”„ Backup strategies**: Have fallback methods for critical data

### âš–ï¸ Legal and Ethical Considerations
1. **ğŸ“‹ Terms of service**: Comply with website terms of service
2. **â±ï¸ Rate limiting**: Don't overload servers with requests
3. **ğŸ›¡ï¸ Data usage**: Use scraped data responsibly and ethically
4. **ğŸ”’ Privacy**: Respect user privacy and data protection laws
5. **ğŸ“ Attribution**: Properly attribute data sources when required

## ğŸ”§ Troubleshooting

### âš ï¸ Common Issues
- **ğŸ¯ Selector failures**: CSS selectors no longer match elements
- **â±ï¸ Rate limiting**: Getting blocked by anti-bot measures
- **ğŸ”„ Data format changes**: Product page structure modifications
- **ğŸ’» JavaScript rendering**: Pages requiring JavaScript execution
- **ğŸŒ Geographic blocking**: Region-specific access restrictions

### ğŸ› ï¸ Debug Tools
```bash
# ğŸ§ª Test individual product scraping
python -c "from src.scrapers.amazon.amazon_scraper import AmazonScraper; print(AmazonScraper().scrape_product('URL'))"

# âœ… Validate scraper configuration
python -c "from src.scrapers.base_scraper import BaseScraper; BaseScraper().test_configuration()"

# ğŸ—‘ï¸ Check response caching
python -c "from src.scrapers.target.target_scraper import TargetScraper; TargetScraper().clear_cache()"
```

### ğŸ“Š Performance Monitoring
- â±ï¸ **Response times**: Track average response times per retailer
- âœ… **Success rates**: Monitor successful vs. failed scraping attempts
- ğŸ“‹ **Data completeness**: Measure percentage of complete product records
- ğŸ” **Error patterns**: Identify recurring error types and patterns 