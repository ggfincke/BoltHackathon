# ğŸ›’ TrackBasket

A comprehensive web crawling system for extracting product data from major retail websites (Amazon, Target, Walmart) with advanced features including UPC lookup, multiple output backends (JSON files, Redis queues, Supabase database), and hierarchical category processing.

## ğŸ“ Project Structure

```
â”œâ”€â”€ src/                        # ğŸ’» Source code
â”‚   â”œâ”€â”€ crawlers/              # ğŸ•·ï¸ Web crawlers for discovering and extracting data
â”‚   â”‚   â”œâ”€â”€ base_crawler.py    # ğŸ—ï¸ Base classes and interfaces
â”‚   â”‚   â”œâ”€â”€ supabase_backend.py # ğŸ—„ï¸ Supabase database backend
â”‚   â”‚   â”œâ”€â”€ upc_lookup/        # ğŸ” UPC/barcode lookup functionality
â”‚   â”‚   â”‚   â”œâ”€â”€ base_upc_lookup.py    # ğŸ¯ Base UPC lookup interface
â”‚   â”‚   â”‚   â”œâ”€â”€ barcode_lookup.py     # ğŸ“Š BarcodeLookup.com service
â”‚   â”‚   â”‚   â””â”€â”€ upc_manager.py        # ğŸ›ï¸ Multi-service UPC manager
â”‚   â”‚   â”œâ”€â”€ normalizers/       # ğŸ·ï¸ Category normalization utilities
â”‚   â”‚   â”œâ”€â”€ amazon/            # ğŸ“¦ Amazon-specific crawler implementation
â”‚   â”‚   â”œâ”€â”€ target/            # ğŸ¯ Target-specific crawler implementation
â”‚   â”‚   â””â”€â”€ walmart/           # ğŸª Walmart-specific crawler implementation
â”‚   â”œâ”€â”€ scrapers/              # ğŸ§¹ Direct product scrapers (alternative approach)
â”‚   â”‚   â”œâ”€â”€ base_scraper.py    # ğŸ—ï¸ Base scraper class
â”‚   â”‚   â”œâ”€â”€ amazon/            # ğŸ“¦ Amazon product scraper
â”‚   â”‚   â”œâ”€â”€ target/            # ğŸ¯ Target product scraper
â”‚   â”‚   â””â”€â”€ walmart/           # ğŸª Walmart product scraper
â”‚   â”œâ”€â”€ data_processing/       # âš™ï¸ Data processing utilities (future expansion)
â”‚   â””â”€â”€ utils/                 # ğŸ”§ Shared utilities (future expansion)
â”œâ”€â”€ scripts/                   # ğŸš€ Executable scripts
â”‚   â”œâ”€â”€ crawl.py              # ğŸ® Main crawler CLI interface
â”‚   â””â”€â”€ crawl.sh              # ğŸš Shell script wrapper
â”œâ”€â”€ data/                      # ğŸ’¾ Data storage
â”‚   â”œâ”€â”€ raw/                  # ğŸ“¥ Raw data files
â”‚   â””â”€â”€ processed/            # ğŸ“¤ Processed data outputs and hierarchy files
â”‚       â”œâ”€â”€ amazon_grocery_hierarchy.json    # ğŸ“¦ Pre-built Amazon categories
â”‚       â”œâ”€â”€ target_grocery_hierarchy.json    # ğŸ¯ Pre-built Target categories
â”‚       â”œâ”€â”€ walmart_grocery_hierarchy.json   # ğŸª Pre-built Walmart categories
â”‚       â””â”€â”€ categories.json                  # ğŸ·ï¸ General category mappings
â”œâ”€â”€ config/                    # âš™ï¸ Configuration files (future expansion)
â”œâ”€â”€ docs/                      # ğŸ“š Additional documentation
â”‚   â””â”€â”€ README.md             # ğŸ“– Detailed crawler documentation
â”œâ”€â”€ supabase/                  # ğŸ—„ï¸ Supabase configuration
â”‚   â”œâ”€â”€ migrations/           # ğŸ”„ Database migrations
â”‚   â””â”€â”€ config.toml           # âš™ï¸ Supabase configuration
â”œâ”€â”€ temp/                      # ğŸ—‚ï¸ Temporary files and utilities
â”œâ”€â”€ test.py                   # ğŸ§ª Test scripts
â”œâ”€â”€ requirements.txt          # ğŸ“‹ Python dependencies
â””â”€â”€ README.md                 # ğŸ“„ This file
```

## ğŸš€ Quick Start

### ğŸ“¥ Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd trackbasket
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set up environment variables (optional):
```bash
export REDIS_URL="redis://localhost:6379"
export SUPABASE_URL="your-supabase-url"
export SUPABASE_ANON_KEY="your-supabase-key"
```

### ğŸ¯ Basic Usage

```bash
# ğŸ“‹ List available retailers
python scripts/crawl.py --list-retailers

# ğŸ”Œ Test connections
python scripts/crawl.py --test-redis

# ğŸ›ï¸ Basic crawling examples
python scripts/crawl.py --retailer amazon --mode full --max-pages 3
python scripts/crawl.py --retailer target --mode urls-only --category "Beverages"
python scripts/crawl.py --retailer walmart --mode full --hierarchical
python scripts/crawl.py --retailer amazon --from-hierarchy-file --concurrency 10
```

## ğŸ”‘ Key Features

### ğŸ•·ï¸ Crawlers vs ğŸ§¹ Scrapers

- **ğŸ•·ï¸ Crawlers** (`src/crawlers/`): Discover product categories and URLs, then extract product data at scale with UPC enrichment
- **ğŸ§¹ Scrapers** (`src/scrapers/`): Direct product data extraction from individual product pages

### ğŸ” UPC Lookup
The system includes sophisticated UPC/barcode lookup capabilities:
- **ğŸ”„ Multi-service fallback**: Automatically tries multiple UPC lookup services
- **ğŸ’¾ Intelligent caching**: Reduces redundant API calls and improves performance  
- **â­ Confidence scoring**: Evaluates the reliability of UPC matches
- **ğŸ¯ Product name matching**: Uses fuzzy matching to find UPCs for product names
  
### ğŸ“¤ Output Backends

1. **ğŸ“„ JSON Files**: Store data in JSON format (default for full mode)
2. **âš¡ Redis**: Store URLs in Redis queues (default for urls-only mode)  
3. **ğŸ—„ï¸ Supabase**: Store structured data directly in database with automatic UPC lookup

### ğŸ› ï¸ Crawling Modes

- **ğŸ” Full Mode**: Extract complete product data (title, price, URL, UPC when available)
- **ğŸ”— URLs Only**: Extract only product URLs for memory efficiency
- **ğŸŒ³ Hierarchical**: Build category structure with products at leaf nodes

## ğŸš€ Advanced Features

### ğŸ“Š Hierarchy File Mode
Use pre-built category hierarchies for faster, more targeted crawling:

```bash
# âš¡ Use default hierarchy file with high concurrency
python scripts/crawl.py --retailer amazon --from-hierarchy-file --mode urls-only --concurrency 15

# ğŸ“ Use custom hierarchy file
python scripts/crawl.py --retailer target --from-hierarchy-file my_hierarchy.json --mode full

# ğŸ¯ Filter hierarchy to specific categories
python scripts/crawl.py --retailer walmart --from-hierarchy-file --category "Beverages" --mode full
```

### ğŸ·ï¸ Category Filtering
Target specific categories or departments:

```bash
python scripts/crawl.py --retailer walmart --category "Beverages" --mode full
python scripts/crawl.py --retailer amazon --department "Amazon Grocery" --hierarchical
python scripts/crawl.py --retailer target --department "Target Grocery" --max-pages 10
```

### âš¡ Concurrent Processing
Optimize performance with adjustable concurrency:

```bash
python scripts/crawl.py --retailer target --from-hierarchy-file --concurrency 20 --max-pages 5
```

### ğŸ“¤ Advanced Output Options
Control output format and destination:

```bash
# ğŸ“ Custom output file names
python scripts/crawl.py --retailer amazon --mode full --output custom_amazon_crawl

# ğŸŒ³ Hierarchical JSON output
python scripts/crawl.py --retailer walmart --hierarchical --output walmart_hierarchy
```

## ğŸ“Š Data Outputs

### ğŸ“„ JSON Files
- **ğŸ“ Location**: Project root directory
- **ğŸ“‹ Format**: ND-JSON (one object per line) or hierarchical JSON
- **ğŸ·ï¸ Naming**: `{prefix}_{timestamp}.json`
- **ğŸ“¦ Content**: Product data with UPC codes when available

### âš¡ Redis Queues
- **ğŸ”‘ Keys**: `product_urls:{retailer_id}`
- **ğŸ“‹ Format**: Plain URLs as strings
- **ğŸ¯ Use case**: URL collection for batch processing

### ğŸ—„ï¸ Supabase Database
- **ğŸ“Š Tables**: products, listings, categories, brands, price_histories, upcs
- **âœ¨ Features**: Automatic deduplication, category normalization, price tracking, UPC enrichment

## âš™ï¸ Configuration

### ğŸŒ Environment Variables
- `CRAWLER_MAX_DEPTH`: Maximum crawling depth (default: 10)
- `CRAWLER_CONCURRENCY`: Number of concurrent workers (default: 5)
- `REDIS_URL`: Redis connection URL
- `SUPABASE_URL`: Supabase project URL
- `SUPABASE_ANON_KEY`: Supabase anonymous key

### ğŸ“Š Pre-built Category Hierarchies
Each retailer has optimized category hierarchy files in `data/processed/`:
- ğŸ“¦ Amazon: `amazon_grocery_hierarchy.json` (7,335 lines)
- ğŸ¯ Target: `target_grocery_hierarchy.json` (1,017 lines)  
- ğŸª Walmart: `walmart_grocery_hierarchy.json` (3,329 lines)

### ğŸ” UPC Lookup Configuration
- **ğŸ“Š BarcodeLookup.com**: Primary UPC lookup service with fuzzy matching
- **ğŸ’¾ Caching**: Intelligent caching of both positive and negative results
- **ğŸ”„ Fallback**: Automatic fallback to additional services when available

## ğŸ® CLI Reference

### ğŸ”§ Main Arguments
- `--retailer, -r`: Choose retailer (amazon, target, walmart)
- `--mode, -m`: Crawling mode (full, urls-only)
- `--hierarchical`: Build/output hierarchical structure
- `--from-hierarchy-file [FILE]`: Use pre-built hierarchy (much faster)

### ğŸ¯ Filtering Options
- `--department, -d`: Target specific department
- `--category, -c`: Target specific category  
- `--max-pages, -p`: Limit pages per category (default: 5)

### âš¡ Performance Options
- `--concurrency`: Concurrent workers (default: 5, recommended: 10-20 for hierarchy mode)
- `--output, -o`: Custom output file prefix
- `--log-level, -l`: Logging verbosity (DEBUG, INFO, WARNING, ERROR)

### ğŸ”§ Utility Commands
- `--list-retailers`: Show available retailers and their configurations
- `--test-redis`: Test Redis connectivity

## ğŸ“¦ Dependencies

Key Python packages required:
- `redis` - âš¡ Redis queue backend
- `pydantic` - âœ… Data validation and serialization
- `requests` - ğŸŒ HTTP client for web requests
- `beautifulsoup4` - ğŸœ HTML parsing
- `selenium` - ğŸ¤– Browser automation
- `playwright` - ğŸ­ Modern browser automation
- `undetected-chromedriver` - ğŸ•µï¸ Anti-detection browser driver
- `easyocr` - ğŸ‘ï¸ OCR for image-based UPC extraction
- `torch`, `opencv-python` - ğŸ¤– Computer vision dependencies

## ğŸ› ï¸ Development

### ğŸ—ï¸ Project Structure Benefits
- **ğŸ¯ Clear separation of concerns**: Crawlers, scrapers, and utilities are organized separately
- **ğŸ§© Modular design**: Easy to add new retailers or UPC lookup services
- **ğŸ“ˆ Scalable architecture**: Supports multiple output backends and processing modes
- **ğŸ”„ No circular dependencies**: Clean import structure

### ğŸ†• Adding New Retailers
1. Create retailer directory in `src/crawlers/`
2. Implement crawler class inheriting from `BaseCrawler`
3. Add configuration to `scripts/crawl.py` `RETAILER_CONFIG`
4. Create category hierarchy file in `data/processed/`

### ğŸ” Adding UPC Lookup Services
1. Implement service class inheriting from `BaseUPCLookup`
2. Add to `UPCManager._initialize_default_services()`
3. Configure fallback priority and caching behavior

### ğŸ§ª Testing
```bash
python test.py
```

## âš¡ Performance Tips

### ğŸš€ Fastest Crawling
- Use `--from-hierarchy-file` mode (10x faster than discovery crawling)
- Set `--concurrency 15-20` for hierarchy mode
- Use `--mode urls-only` for maximum memory efficiency

### ğŸ­ Production Deployment
- Enable Redis for URL queue management
- Configure Supabase for structured data storage
- Monitor UPC lookup cache hit rates for cost optimization

### ğŸ’¾ Memory Optimization
- Use URLs-only mode for large crawls
- Process data in batches via Redis queues
- Clear UPC lookup cache periodically in long-running processes

## ğŸ”§ Troubleshooting

### â— Common Issues
1. **ğŸ“¥ Import errors**: Ensure you're running scripts from the project root
2. **ğŸ“¦ Missing dependencies**: Install all requirements, especially computer vision packages
3. **ğŸ” UPC lookup failures**: Check internet connectivity and service rate limits
4. **ğŸ’¾ High memory usage**: Use urls-only mode or reduce concurrency

### âš¡ Performance Issues
- Monitor UPC lookup service response times
- Adjust concurrency based on system resources and target site limits  
- Use hierarchy files instead of discovery crawling when possible

## ğŸ“„ License

[Add your license information here]

## ğŸ¤ Contributing

[Add contribution guidelines here] 